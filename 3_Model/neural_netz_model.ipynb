{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Import Data\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/house_pricing_data/house_pricing_train.csv\")\n",
    "data.head()  # Print first few rows to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical features\n",
    "categorical_features = ['bathrooms', 'condition']\n",
    "\n",
    "# Inspect data types and unique values for categorical columns\n",
    "print(data[categorical_features].dtypes)\n",
    "print(\"Unique Values:\\n\",data[categorical_features].apply(lambda x: x.unique()))\n",
    "\n",
    "# Ensure categorical columns are treated as categories\n",
    "for col in categorical_features:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "# Encode categorical variables using pd.get_dummies\n",
    "features = pd.get_dummies(data[categorical_features], drop_first=True, dtype=int)\n",
    "\n",
    "# Include any numeric columns that are not categorical\n",
    "features['sqft_living15'] = data['sqft_living15']\n",
    "\n",
    "# Construct the prepared data set including the dependent variable ('label')\n",
    "prepared_data = pd.concat([data[['price']], features], axis=1)\n",
    "\n",
    "# Handle missing values by removing rows with any missing values\n",
    "prepared_data = prepared_data.dropna()\n",
    "\n",
    "# Display the shape of the prepared data set\n",
    "print(prepared_data.shape)\n",
    "# Display the first few rows of the prepared data set\n",
    "prepared_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "prepared_data = prepared_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Calculate the number of rows for each dataset\n",
    "n_total = len(prepared_data)\n",
    "n_training = int(0.7 * n_total)\n",
    "n_validation = int(0.20 * n_total)\n",
    "\n",
    "# Split the features and labels for training, validation, and test\n",
    "training_data = prepared_data.iloc[:n_training]\n",
    "validation_data = prepared_data.iloc[n_training:n_training+n_validation]\n",
    "test_data = prepared_data.iloc[n_training+n_validation:]\n",
    "\n",
    "# Separating features and labels\n",
    "training_features = training_data.drop('price', axis=1)\n",
    "validation_features = validation_data.drop('price', axis=1)\n",
    "test_features = test_data.drop('price', axis=1)\n",
    "\n",
    "training_labels = training_data[['price']]\n",
    "validation_labels = validation_data[['price']]\n",
    "test_labels = test_data[['price']]\n",
    "\n",
    "# Print dimensions of the dataframes\n",
    "print(\"Training features dimensions:\", training_features.shape)\n",
    "print(\"Validation features dimensions:\", validation_features.shape)\n",
    "print(\"Test features dimensions:\", test_features.shape)\n",
    "print()\n",
    "print(\"Training labels dimensions:\", training_labels.shape)\n",
    "print(\"Validation labels dimensions:\", validation_labels.shape)\n",
    "print(\"Test labels dimensions:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subdirectory for the pickle files\n",
    "subdirectory = \"pickle_data\"\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Export of the prepared data to subdirectory as pickle files\n",
    "training_features.to_pickle(f\"{subdirectory}/training_features.pkl\")\n",
    "validation_features.to_pickle(f\"{subdirectory}/validation_features.pkl\")\n",
    "test_features.to_pickle(f\"{subdirectory}/test_features.pkl\")\n",
    "training_labels.to_pickle(f\"{subdirectory}/training_labels.pkl\")\n",
    "validation_labels.to_pickle(f\"{subdirectory}/validation_labels.pkl\")\n",
    "test_labels.to_pickle(f\"{subdirectory}/test_labels.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "subdirectory = \"pickle_data\"\n",
    "training_features_path = f\"{subdirectory}/training_features.pkl\"\n",
    "validation_features_path = f\"{subdirectory}/validation_features.pkl\"\n",
    "test_features_path = f\"{subdirectory}/test_features.pkl\"\n",
    "training_labels_path = f\"{subdirectory}/training_labels.pkl\"\n",
    "validation_labels_path = f\"{subdirectory}/validation_labels.pkl\"\n",
    "test_labels_path = f\"{subdirectory}/test_labels.pkl\"\n",
    "\n",
    "# Read the pickle files\n",
    "training_features = pd.read_pickle(training_features_path)\n",
    "validation_features = pd.read_pickle(validation_features_path)\n",
    "test_features = pd.read_pickle(test_features_path)\n",
    "training_labels = pd.read_pickle(training_labels_path)\n",
    "validation_labels = pd.read_pickle(validation_labels_path)\n",
    "test_labels = pd.read_pickle(test_labels_path)\n",
    "\n",
    "# Verify the loaded data by printing their shapes and a few rows\n",
    "print(\"Loaded Training features dimensions:\", training_features.shape)\n",
    "print(\"Loaded Validation features dimensions:\", validation_features.shape)\n",
    "print(\"Loaded Test features dimensions:\", test_features.shape)\n",
    "print()\n",
    "print(\"Loaded Training labels dimensions:\", training_labels.shape)\n",
    "print(\"Loaded Validation labels dimensions:\", validation_labels.shape)\n",
    "print(\"Loaded Test labels dimensions:\", test_labels.shape)\n",
    "print()\n",
    "\n",
    "print(\"First few rows of loaded training features:\")\n",
    "print(training_features.head())\n",
    "print()\n",
    "print(\"First few rows of loaded training labels:\")\n",
    "print(training_labels.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "  InputLayer(shape=(training_features.shape[1], )),\n",
    "  BatchNormalization(),\n",
    "  Dense(10, activation='relu'),\n",
    "  Dense(4, activation='relu'),\n",
    "  Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "history = model.fit(training_features, training_labels, epochs=20,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"python_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "\n",
    "training_predictions = model.predict(training_features)\n",
    "validation_predictions = model.predict(validation_features)\n",
    "print(f\"MAPE on the Training Data: {mape(training_labels, training_predictions):.2f}%\")\n",
    "print(f\"MAPE on the Validation Data: {mape(validation_labels, validation_predictions):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(data, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(data['actual'], label='Actual Values', color='red')\n",
    "    plt.plot(data['prediction'], label='Predicted Values', color='blue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Case Number')\n",
    "    plt.ylabel('Price in 1.000 USD')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Ensure that training_predictions, validation_predictions, training_labels, and validation_labels are numpy arrays\n",
    "training_predictions = np.array(training_predictions).flatten()\n",
    "validation_predictions = np.array(validation_predictions).flatten()\n",
    "training_labels = np.array(training_labels).flatten()\n",
    "validation_labels = np.array(validation_labels).flatten()\n",
    "\n",
    "# print the type of the predictions\n",
    "print(type(training_predictions))\n",
    "print(type(validation_predictions))\n",
    "\n",
    "# Create DataFrames with 1-dimensional arrays\n",
    "data_train = pd.DataFrame({'prediction': training_predictions, 'actual': training_labels})\n",
    "data_validation = pd.DataFrame({'prediction': validation_predictions, 'actual': validation_labels})\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(data_train.head(100), 'Predicted and Actual Values for the Training Data')\n",
    "plot_predictions(data_validation.head(100), 'Predicted and Actual Values for the Validation Data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
