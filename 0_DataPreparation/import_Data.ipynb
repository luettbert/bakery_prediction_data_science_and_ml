{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titel: Import data\n",
    "## Author: Achraf Aboukinana\n",
    "\n",
    "## Beschreibung des Codes\n",
    "\n",
    "Dieser Code dient der Analyse, Bereinigung und Kombination von Datensätzen, um eine zentrale und bereinigte Datenbasis zu erstellen. Die Hauptaufgaben sind wie folgt:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenimport mit Pandas und Feiertagsbibliothek\n",
    "\n",
    "Dieses Skript lädt verschiedene Datensätze ein, um sie später für Analysen oder Modellierungsschritte zu verwenden. \n",
    "\n",
    "## Importierte Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_pfad= 'kaggel/test.csv'\n",
    "model_data_pfad = 'final_data_test.csv'\n",
    "# Import Data\n",
    "kiwo_data = pd.read_csv('kaggel/kiwo.csv', parse_dates=['Datum'])\n",
    "umsatzdaten_gekuerzt_data = pd.read_csv(data_pfad, parse_dates=['Datum'])\n",
    "\n",
    "wetter_data = pd.read_csv('kaggel/wetter.csv', parse_dates=['Datum'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenzusammenführung und Ergänzung der KielerWoche-Information\n",
    "\n",
    "Dieses Skript kombiniert Umsatz-, Wetter- und KielerWoche-Daten und ergänzt fehlende Informationen, um eine vollständige Datengrundlage für Analysen oder Modelle zu erstellen.\n",
    "\n",
    "## Schritte der Datenverarbeitung\n",
    "\n",
    "### 1. Zusammenführen von Umsatz- und Wetterdaten\n",
    "Die `umsatzdaten_gekuerzt_data` und `wetter_data` werden auf der gemeinsamen Spalte `Datum` mit einem **Left Join** zusammengeführt.\n",
    "\n",
    "### 2. Hinzufügen von KielerWoche-Information\n",
    "Die `kiwo_data` wird mit den bereits zusammengeführten Daten (`merged_data`) auf der Spalte `Datum` kombiniert. Fehlende Werte in `KielerWoche` werden durch `0` ersetzt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import holidays\n",
    "\n",
    "# Merge Umsatzdaten mit Wetterdaten auf 'Datum'\n",
    "merged_data = pd.merge(umsatzdaten_gekuerzt_data, wetter_data, on='Datum', how='left')\n",
    "\n",
    "# Füge KielerWoche-Information hinzu und ersetze fehlende Werte durch 0\n",
    "final_data = pd.merge(merged_data, kiwo_data, on='Datum', how='left')\n",
    "final_data['KielerWoche'] = final_data['KielerWoche'].fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umgang mit fehlenden Werten (NaN)\n",
    "\n",
    "In diesem Abschnitt wird überprüft, wie viele fehlende Werte (`NaN`) in den Daten vorhanden sind, und anschließend werden diese Werte behandelt, um saubere Daten für die Analyse oder Modellierung zu erhalten.\n",
    "\n",
    "## Schritte der Datenverarbeitung\n",
    "\n",
    "### 1. **Überprüfung der Anzahl der NaN-Werte**\n",
    "Mit der Methode `isna().sum()` wird ermittelt, wie viele fehlende Werte (`NaN`) in jeder Spalte des DataFrame `final_data` vorhanden sind. Das Ergebnis wird für jede Spalte separat angezeigt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der NaN-Werte pro Spalte:\n",
      "id                       0\n",
      "Datum                    0\n",
      "Warengruppe              0\n",
      "Bewoelkung              65\n",
      "Temperatur              65\n",
      "Windgeschwindigkeit     65\n",
      "Wettercode             337\n",
      "KielerWoche              0\n",
      "dtype: int64\n",
      "1493\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Anzahl der NaN-Werte überprüfen\n",
    "nan_counts = final_data.isna().sum()\n",
    "print(\"Anzahl der NaN-Werte pro Spalte:\")\n",
    "print(nan_counts)\n",
    "\n",
    "# Entferne Zeilen mit NaN-Werten\n",
    "clean_data = final_data.dropna()\n",
    "print(len(clean_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hinzufügen zeitabhängiger Variablen\n",
    "\n",
    "In diesem Abschnitt werden zeitbezogene Merkmale zu den Daten hinzugefügt. Diese Variablen können in Analysen oder Modellierungen hilfreich sein, um saisonale oder zeitliche Muster zu erkennen.\n",
    "\n",
    "## Schritte der Datenverarbeitung\n",
    "\n",
    "### 1. **Wochentag hinzufügen**\n",
    "Die Spalte `Wochentag` wird basierend auf der Spalte `Datum` berechnet. \n",
    "- **Wertebereich**: Montag = 0, Dienstag = 1, ..., Sonntag = 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lorda\\AppData\\Local\\Temp\\ipykernel_24476\\1988144829.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['Wochentag'] = clean_data['Datum'].dt.dayofweek  # Montag=0, Sonntag=6\n",
      "C:\\Users\\lorda\\AppData\\Local\\Temp\\ipykernel_24476\\1988144829.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['Monat'] = clean_data['Datum'].dt.month\n",
      "C:\\Users\\lorda\\AppData\\Local\\Temp\\ipykernel_24476\\1988144829.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['Jahr'] = clean_data['Datum'].dt.year\n",
      "C:\\Users\\lorda\\AppData\\Local\\Temp\\ipykernel_24476\\1988144829.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['Jahreszeit'] = clean_data['Datum'].dt.month % 12 // 3 + 1  # Frühling=1, Sommer=2, Herbst=3, Winter=4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# **Zeitabhängige Variablen hinzufügen**\n",
    "clean_data['Wochentag'] = clean_data['Datum'].dt.dayofweek  # Montag=0, Sonntag=6\n",
    "clean_data['Monat'] = clean_data['Datum'].dt.month\n",
    "clean_data['Jahr'] = clean_data['Datum'].dt.year\n",
    "\n",
    "clean_data['Jahreszeit'] = clean_data['Datum'].dt.month % 12 // 3 + 1  # Frühling=1, Sommer=2, Herbst=3, Winter=4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hinzufügen von Jahreszeiten, Temperaturempfindungen und Dummy-Variablen\n",
    "\n",
    "Dieser Abschnitt erweitert die Daten um zusätzliche Features basierend auf Jahreszeiten und Temperatur, gefolgt von einem Dummy-Encoding für die Temperaturkategorien.\n",
    "\n",
    "## Schritte der Datenverarbeitung\n",
    "\n",
    "### 1. **Bestimmung der Jahreszeit**\n",
    "Eine Funktion `get_season` wird definiert, um basierend auf dem Monat (`Monat`) die entsprechende Jahreszeit zu bestimmen:\n",
    "- **Winter**: Dezember, Januar, Februar,\n",
    "- **Frühling**: März, April, Mai,\n",
    "- **Sommer**: Juni, Juli, August,\n",
    "- **Herbst**: September, Oktober, November.\n",
    "\n",
    "Die Jahreszeit wird in der neuen Spalte `Jahreszeit` gespeichert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lorda\\AppData\\Local\\Temp\\ipykernel_24476\\1434982144.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['Jahreszeit'] = clean_data['Monat'].apply(get_season)\n",
      "C:\\Users\\lorda\\AppData\\Local\\Temp\\ipykernel_24476\\1434982144.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['Gefühl'] = clean_data.apply(lambda x: temperature_feeling(x['Temperatur'], x['Jahreszeit']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Jahreszeit bestimmen\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Frühling\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Sommer\"\n",
    "    elif month in [9, 10, 11]:\n",
    "        return \"Herbst\"\n",
    "\n",
    "clean_data['Jahreszeit'] = clean_data['Monat'].apply(get_season)\n",
    "\n",
    "# Gefühl abhängig von Temperatur und Jahreszeit\n",
    "def temperature_feeling(temp, season):\n",
    "    if season == \"Winter\":\n",
    "        if temp <= 0:\n",
    "            return \"Kalt\"\n",
    "        elif temp <= 10:\n",
    "            return \"Mild\"\n",
    "        else:\n",
    "            return \"Warm\"\n",
    "    elif season == \"Frühling\":\n",
    "        if temp <= 10:\n",
    "            return \"Kalt\"\n",
    "        elif temp <= 20:\n",
    "            return \"Mild\"\n",
    "        else:\n",
    "            return \"Warm\"\n",
    "    elif season == \"Sommer\":\n",
    "        if temp <= 15:\n",
    "            return \"Kalt\"\n",
    "        elif temp <= 25:\n",
    "            return \"Mild\"\n",
    "        else:\n",
    "            return \"Warm\"\n",
    "    elif season == \"Herbst\":\n",
    "        if temp <= 10:\n",
    "            return \"Kalt\"\n",
    "        elif temp <= 20:\n",
    "            return \"Mild\"\n",
    "        else:\n",
    "            return \"Warm\"\n",
    "\n",
    "clean_data['Gefühl'] = clean_data.apply(lambda x: temperature_feeling(x['Temperatur'], x['Jahreszeit']), axis=1)\n",
    "\n",
    "# Dummy-Encoding\n",
    "gefühl_dummies = pd.get_dummies(clean_data['Gefühl'],prefix='gefühl')\n",
    "print(len(clean_data))\n",
    "# Dummy-Variablen anhängen und Originalspalten löschen\n",
    "clean_data = pd.concat([clean_data,gefühl_dummies], axis=1)\n",
    "#gefühl_dummies.drop(columns=['Gefühl'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hinzufügen von Feiertagen\n",
    "\n",
    "In diesem Abschnitt werden Feiertage für jedes Datum im Datensatz hinzugefügt. Dies hilft, Feiertagseffekte zu analysieren und sie als zusätzliche Merkmale in Modellen zu verwenden.\n",
    "\n",
    "## Schritte der Datenverarbeitung\n",
    "\n",
    "### 1. **Feiertage abrufen**\n",
    "Die Funktion `get_feiertage` verwendet die `holidays`-Bibliothek, um alle Feiertage für ein bestimmtes Jahr in Deutschland abzurufen. Das Ergebnis ist eine Liste von Feiertagen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Feiertage hinzufügen**\n",
    "def get_feiertage(year):\n",
    "    de_holidays = holidays.Germany(years=year)\n",
    "    return list(de_holidays.keys())\n",
    "\n",
    "# Feiertagsinformationen für alle Jahre im Datensatz\n",
    "feiertage_set = {holiday for year in clean_data['Datum'].dt.year.unique() for holiday in get_feiertage(year)}\n",
    "\n",
    "# Feiertagsspalte erstellen\n",
    "clean_data['Feiertag'] = clean_data['Datum'].apply(lambda x: 1 if x.date() in feiertage_set else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hinzufügen von besonderen Events (KielLauf, Kieler Triathlon, wichtige Fußballspiele)\n",
    "\n",
    "In diesem Abschnitt werden besondere Events (wie der KielLauf, der Kieler Triathlon und wichtige Fußballspiele) als zusätzliche Merkmale zum Datensatz hinzugefügt. Diese Events können für die Analyse von Trends oder Anomalien an spezifischen Tagen verwendet werden.\n",
    "\n",
    "## Schritte der Datenverarbeitung\n",
    "\n",
    "### 1. **Besondere Ereignisdaten**\n",
    "Es werden Listen von Datumsangaben für besondere Ereignisse erstellt:\n",
    "\n",
    "- **Kieler Triathlon:** Ein Ereignis, das jährlich stattfindet (vom Jahr 2013 bis 2019).\n",
    "- **KielLauf:** Ein weiteres jährliches Ereignis von 2013 bis 2019.\n",
    "- **Wichtige Fußballspiele:** Ein Satz von bedeutenden Spielen der deutschen Fußball-Nationalmannschaft, darunter Spiele der FIFA-Weltmeisterschaft 2014, der UEFA Euro 2016, des FIFA-Konföderationen-Pokals 2017, der FIFA-Weltmeisterschaft 2018 und der Olympischen Spiele 2016.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# **Besondere Events (KielLauf, Kieler Triathlon) hinzufügen**\n",
    "kieler_triathlon_daten = pd.to_datetime(['2013-08-04', '2014-08-03', '2015-08-02', '2016-08-07', '2017-08-06', \n",
    "                                         '2018-08-05', '2019-08-04'])\n",
    "kiellauf_daten = pd.to_datetime([\"2013-09-08\", \"2014-09-14\", \"2015-09-13\", \"2016-09-11\", \n",
    "                                 \"2017-09-10\", \"2018-09-09\", \"2019-09-08\"])\n",
    "important_games_germany = pd.to_datetime([\n",
    "    # FIFA World Cup 2014\n",
    "    \"2014-07-08\",  # Germany 7–1 Brazil (Semifinal)\n",
    "    \"2014-07-13\",  # Germany 1–0 Argentina (Final)\n",
    "\n",
    "    # UEFA Euro 2016\n",
    "    \"2016-07-02\",  # Germany 1–1 Italy (6–5 on penalties, Quarterfinal)\n",
    "    \"2016-07-07\",  # Germany 0–2 France (Semifinal)\n",
    "\n",
    "    # FIFA Confederations Cup 2017\n",
    "    \"2017-06-19\",  # Germany 3–2 Australia\n",
    "    \"2017-06-22\",  # Germany 1–1 Chile\n",
    "    \"2017-06-25\",  # Germany 3–1 Cameroon\n",
    "    \"2017-06-29\",  # Germany 4–1 Mexico (Semifinal)\n",
    "    \"2017-07-02\",  # Germany 1–0 Chile (Final)\n",
    "\n",
    "    # FIFA World Cup 2018\n",
    "    \"2018-06-17\",  # Germany 0–1 Mexico (Group stage)\n",
    "    \"2018-06-27\",  # Germany 0–2 South Korea (Group stage, eliminated)\n",
    "\n",
    "    # Olympic Games 2016 (Men's Football)\n",
    "    \"2016-08-20\"   # Germany 1–1 Brazil (4–5 on penalties, Final)\n",
    "])\n",
    "\n",
    "clean_data['KielLauf'] = clean_data['Datum'].apply(lambda x: 1 if x.date() in kiellauf_daten.date else 0)\n",
    "clean_data['Kieler_Triathlon'] = clean_data['Datum'].apply(lambda x: 1 if x.date() in kieler_triathlon_daten.date else 0)\n",
    "clean_data['Fußball'] = clean_data['Datum'].apply(lambda x: 1 if x.date() in important_games_germany.date else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaycheckEffect und Inflationsempfindlichkeit hinzufügen\n",
    "\n",
    "In diesem Abschnitt wird die **PaycheckEffect**-Spalte erstellt, die angibt, ob ein bestimmter Tag im Zeitraum von Gehaltszahlungen (typischerweise zwischen dem 27. eines Monats und dem 5. des nächsten Monats) liegt. Zusätzlich wird eine **Inflationsempfindlichkeit**-Spalte auf Basis der Warengruppe erstellt, um zu analysieren, wie empfindlich die Produkte gegenüber Inflation sind. \n",
    "\n",
    "## Schritte der Datenverarbeitung\n",
    "\n",
    "### 1. **PaycheckEffect erstellen**\n",
    "Die PaycheckEffect-Variable wird erstellt, um Tage zu kennzeichnen, an denen Gehaltszahlungen erwartet werden (normalerweise zwischen dem 27. eines Monats und dem 5. des nächsten Monats). Diese Variable hat den Wert `1` an diesen Tagen und `0` an anderen Tagen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **PaycheckEffect (Gehaltszahlungen)**\n",
    "clean_data['PaycheckEffect'] = clean_data['Datum'].apply(\n",
    "    lambda x: 1 if x.day >= 27 or x.day <= 5 else 0\n",
    ")\n",
    "\n",
    "# Inflation sensitivity mapping based on WarenGruppe\n",
    "sensitivity_mapping = {\n",
    "    1: \"High\",   # Brot\n",
    "    2: \"High\",   # Brötchen\n",
    "    3: \"Moderate\",  # Croissant\n",
    "    4: \"Moderate\",  # Konditorei\n",
    "    5: \"Moderate\",  # Kuchen\n",
    "    6: \"High\"    # Saisonbrot\n",
    "}\n",
    "\n",
    "# Map the sensitivity to the WarenGruppe\n",
    "clean_data[\"InflationSensitivity\"] = clean_data[\"Warengruppe\"].map(sensitivity_mapping)\n",
    "\n",
    "# Perform dummy encoding for sensitivity levels, excluding 'Low'\n",
    "dummy_encoded = pd.get_dummies(clean_data[\"InflationSensitivity\"], prefix=\"Sensitivity\").drop(columns=[\"Sensitivity_Low\"], errors=\"ignore\")\n",
    "\n",
    "# Concatenate the dummy columns with the original DataFrame\n",
    "clean_data = pd.concat([clean_data, dummy_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windkategorie basierend auf Windgeschwindigkeit\n",
    "\n",
    "In diesem Abschnitt wird eine **Windkategorie** erstellt, die auf der Windgeschwindigkeit basiert. Die Windgeschwindigkeiten werden in drei Kategorien unterteilt:\n",
    "- **Nicht_windig**: Windgeschwindigkeit < 10 km/h\n",
    "- **Windig**: Windgeschwindigkeit zwischen 10 und 20 km/h\n",
    "- **Sehr_windig**: Windgeschwindigkeit > 20 km/h\n",
    "\n",
    "Nach der Kategorisierung wird die **Dummy-Codierung** angewendet, um die Windkategorien als separate Spalten im Datensatz darzustellen.\n",
    "\n",
    "## Schritte der Datenverarbeitung\n",
    "\n",
    "### 1. **Windkategorie bestimmen**\n",
    "Eine Funktion `wind_kategorie()` wird verwendet, um die Windgeschwindigkeit in die entsprechenden Kategorien zu unterteilen. Diese Funktion wird auf jede Zeile im Datensatz angewendet, um die neue Spalte **Windkategorie** zu erstellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# **Windkategorie basierend auf Windgeschwindigkeit**\n",
    "def wind_kategorie(wind):\n",
    "    if wind < 10:\n",
    "        return \"Nicht_windig\"\n",
    "    elif wind <= 20:\n",
    "        return \"Windig\"\n",
    "    else:\n",
    "        return \"Sehr_windig\" \n",
    "\n",
    "clean_data['Windkategorie'] = clean_data['Windgeschwindigkeit'].apply(wind_kategorie)\n",
    "\n",
    "# Dummy-Variablen für Windkategorien erstellen\n",
    "wind_dummies = pd.get_dummies(clean_data['Windkategorie'], prefix=\"Wind\")\n",
    "clean_data = pd.concat([clean_data, wind_dummies], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy-Encoding für Jahreszeit und Wetterkategorie\n",
    "\n",
    "In diesem Abschnitt wird das **Dummy-Encoding** für die **Jahreszeit** durchgeführt, um die saisonalen Informationen als separate binäre Variablen darzustellen. Jede Jahreszeit (Frühling, Sommer, Herbst, Winter) wird in eine eigene Spalte umgewandelt, die entweder den Wert `1` oder `0` enthält, je nachdem, ob die Zeile zu dieser Jahreszeit gehört.\n",
    "\n",
    "## Schritte der Datenverarbeitung\n",
    "\n",
    "### 1. **Dummy-Encoding für Jahreszeit**\n",
    "Mit der Funktion `pd.get_dummies()` werden für jede mögliche Jahreszeit separate Spalten erstellt. Diese Spalten werden dann zu einem neuen DataFrame hinzugefügt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Dummy-Encoding für Jahreszeit und Wetterkategorie**\n",
    "jahreszeit_dummies = pd.get_dummies(clean_data['Jahreszeit'], prefix=\"Jahreszeit\")\n",
    "\n",
    "# Dummy-Variablen anhängen und Originalspalten löschen\n",
    "clean_data = pd.concat([clean_data, jahreszeit_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ferienkennung hinzufügen\n",
    "\n",
    "In diesem Abschnitt wird eine Spalte hinzugefügt, die angibt, ob ein bestimmtes Datum innerhalb der Schulferien liegt oder nicht. Wenn der Tag innerhalb der Schulferien liegt, wird der Wert `1` zugewiesen, andernfalls `0`. \n",
    "\n",
    "## Schritte der Datenverarbeitung\n",
    "\n",
    "### 1. **Schulferien in Kiel definieren**\n",
    "Die Schulferien für mehrere Jahre werden als Paare von Start- und Enddaten definiert. Diese Daten umfassen die wichtigsten Ferien (Osterferien, Sommerferien, Herbstferien, Weihnachtsferien).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "ferien_kiel = [\n",
    "    # Schulferien 2013\n",
    "    ('2013-01-01', '2013-01-05'),  # Weihnachtsferien\n",
    "    ('2013-03-25', '2013-04-06'),  # Osterferien\n",
    "    ('2013-06-24', '2013-08-03'),  # Sommerferien\n",
    "    ('2013-10-04', '2013-10-19'),  # Herbstferien\n",
    "    ('2013-12-23', '2014-01-04'),  # Weihnachtsferien\n",
    "\n",
    "    # Schulferien 2014\n",
    "    ('2014-04-07', '2014-04-22'),  # Osterferien\n",
    "    ('2014-07-14', '2014-08-23'),  # Sommerferien\n",
    "    ('2014-10-20', '2014-11-01'),  # Herbstferien\n",
    "    ('2014-12-22', '2015-01-06'),  # Weihnachtsferien\n",
    "\n",
    "    # Schulferien 2015\n",
    "    ('2015-04-01', '2015-04-18'),  # Osterferien\n",
    "    ('2015-07-16', '2015-08-29'),  # Sommerferien\n",
    "    ('2015-10-19', '2015-10-31'),  # Herbstferien\n",
    "    ('2015-12-23', '2016-01-06'),  # Weihnachtsferien\n",
    "\n",
    "    # Schulferien 2016\n",
    "    ('2016-03-24', '2016-04-09'),  # Osterferien\n",
    "    ('2016-07-25', '2016-09-03'),  # Sommerferien\n",
    "    ('2016-10-17', '2016-10-29'),  # Herbstferien\n",
    "    ('2016-12-23', '2017-01-07'),  # Weihnachtsferien\n",
    "\n",
    "    # Schulferien 2017\n",
    "    ('2017-04-07', '2017-04-22'),  # Osterferien\n",
    "    ('2017-07-24', '2017-09-02'),  # Sommerferien\n",
    "    ('2017-10-16', '2017-10-28'),  # Herbstferien\n",
    "    ('2017-12-21', '2018-01-06'),  # Weihnachtsferien\n",
    "\n",
    "    # Schulferien 2018\n",
    "    ('2018-03-29', '2018-04-13'),  # Osterferien\n",
    "    ('2018-07-09', '2018-08-18'),  # Sommerferien\n",
    "    ('2018-10-01', '2018-10-13'),  # Herbstferien\n",
    "    ('2018-12-21', '2019-01-04'),  # Weihnachtsferien\n",
    "\n",
    "    # Schulferien 2019\n",
    "    ('2019-04-04', '2019-04-18'),  # Osterferien\n",
    "    ('2019-07-01', '2019-08-10'),  # Sommerferien\n",
    "    ('2019-10-04', '2019-10-19'),  # Herbstferien\n",
    "    ('2019-12-23', '2020-01-06'),  # Weihnachtsferien\n",
    "\n",
    "    # Schulferien 2020\n",
    "    ('2020-03-30', '2020-04-17'),  # Osterferien\n",
    "    ('2020-06-29', '2020-08-08'),  # Sommerferien\n",
    "    ('2020-10-05', '2020-10-17'),  # Herbstferien\n",
    "    ('2020-12-23', '2021-01-06'),  # Weihnachtsferien\n",
    "]\n",
    "\n",
    "# Konvertiere Ferien zu einer Liste von Datumsobjekten\n",
    "ferien_tage = []\n",
    "for start, end in ferien_kiel:\n",
    "    start_date = datetime.strptime(start, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "    while start_date <= end_date:\n",
    "        ferien_tage.append(start_date)\n",
    "        start_date += timedelta(days=1)\n",
    "\n",
    "\n",
    "# Set der Ferientage für schnellen Zugriff\n",
    "ferien_set = set(ferien_tage)\n",
    "\n",
    "# Spalte hinzufügen: 1 für Ferien, 0 für keine Ferien\n",
    "clean_data['Ferien'] = clean_data['Datum'].apply(lambda x: 1 if x in ferien_set else 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neue Spalte basierend auf Wochentagen erstellen\n",
    "\n",
    "In diesem Abschnitt werden zwei neue Spalten basierend auf den Wochentagen erstellt:\n",
    "\n",
    "1. **`Tag_Kategorie`**: Diese Spalte kategorisiert die Tage als \"Montag bis Freitag\" oder \"Wochenende\", je nachdem, ob der Wochentag (repräsentiert durch `Wochentag`) zwischen Montag (0) und Freitag (4) liegt oder ob es ein Samstag (5) oder Sonntag (6) ist.\n",
    "2. **`Wochenende`**: Diese Spalte enthält den Wert `1`, wenn der Wochentag auf ein Wochenende fällt (Samstag oder Sonntag), und `0` für Wochentage (Montag bis Freitag).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Neue Spalte basierend auf Wochentagen erstellen\n",
    "clean_data['Tag_Kategorie'] = clean_data['Wochentag'].apply(lambda x: 'Montag_bis_Freitag' if x < 5 else 'Wochenende')\n",
    "# Neue Spalte für Wochenenden erstellen\n",
    "clean_data['Wochenende'] = clean_data['Wochentag'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflation Daten Subset\n",
    "\n",
    "In diesem Abschnitt laden wir die Inflationsdaten aus der Datei `inflation.csv` und entfernen bestimmte Spalten, um ein Subset der Daten zu erstellen.\n",
    "\n",
    "### Einlesen der Daten\n",
    "Wir lesen die CSV-Datei und geben die ursprünglichen Spalten der Daten aus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Jahr', 'Monat', 'Verbraucherpreisindex', 'Veränderung_Vorjahresmonat',\n",
      "       'Veränderung_Vormonat', 'Inflation_Kategorisierung',\n",
      "       'Inflation_Kategorisierung_Neutral',\n",
      "       'Inflation_Kategorisierung_Positiv',\n",
      "       'Inflation_Kategorisierung_vormonat_Neutral',\n",
      "       'Inflation_Kategorisierung_vormonat_Positiv',\n",
      "       'Inflation_Kategorisierung_vormonat_Neutral.1',\n",
      "       'Inflation_Kategorisierung_vormonat_Positiv.1'],\n",
      "      dtype='object')\n",
      "Index(['Jahr', 'Monat', 'Inflation_Kategorisierung',\n",
      "       'Inflation_Kategorisierung_Neutral',\n",
      "       'Inflation_Kategorisierung_Positiv'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "inflation_data= pd.read_csv('DATA/inflation.csv')\n",
    "print(inflation_data.columns)\n",
    "subset_inflation = inflation_data.drop(columns=[ 'Verbraucherpreisindex', 'Veränderung_Vorjahresmonat',\n",
    "       'Veränderung_Vormonat',\n",
    "       'Inflation_Kategorisierung_vormonat_Neutral',\n",
    "       'Inflation_Kategorisierung_vormonat_Positiv',\n",
    "       'Inflation_Kategorisierung_vormonat_Neutral.1',\n",
    "       'Inflation_Kategorisierung_vormonat_Positiv.1'])\n",
    "print(subset_inflation.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenführen der Inflationsdaten mit den bereinigten Daten\n",
    "\n",
    "In diesem Schritt fügen wir die Inflationsdaten, die wir zuvor gefiltert haben, zu den bereinigten Wetterdaten (`clean_data`) hinzu. Wir führen die beiden DataFrames basierend auf den Spalten `Jahr` und `Monat` zusammen.\n",
    "\n",
    "### Zusammenführen der Daten\n",
    "\n",
    "Die Zusammenführung erfolgt durch einen linken Join, um alle Zeilen aus `clean_data` zu behalten und die entsprechenden Inflationsdaten hinzuzufügen, wenn verfügbar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'Datum', 'Warengruppe', 'Bewoelkung', 'Temperatur',\n",
      "       'Windgeschwindigkeit', 'Wettercode', 'KielerWoche', 'Wochentag',\n",
      "       'Monat', 'Jahr', 'Jahreszeit', 'Gefühl', 'gefühl_Kalt', 'gefühl_Mild',\n",
      "       'gefühl_Warm', 'Feiertag', 'KielLauf', 'Kieler_Triathlon', 'Fußball',\n",
      "       'PaycheckEffect', 'InflationSensitivity', 'Sensitivity_High',\n",
      "       'Sensitivity_Moderate', 'Windkategorie', 'Wind_Nicht windig',\n",
      "       'Wind_Sehr windig', 'Wind_Windig', 'Jahreszeit_Frühling',\n",
      "       'Jahreszeit_Herbst', 'Jahreszeit_Sommer', 'Jahreszeit_Winter', 'Ferien',\n",
      "       'Tag_Kategorie', 'Wochenende', 'Inflation_Kategorisierung',\n",
      "       'Inflation_Kategorisierung_Neutral',\n",
      "       'Inflation_Kategorisierung_Positiv'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clean_data = pd.merge(clean_data, subset_inflation, on=['Jahr', 'Monat'], how='left')\n",
    "print(clean_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy-Encoding für Wochentag und Warengruppe\n",
    "\n",
    "In diesem Schritt erstellen wir Dummy-Variablen für die Spalten `Wochentag` und `Warengruppe`, um diese kategorialen Variablen in numerische Formate zu konvertieren, die für Modelle und Analysen verwendet werden können.\n",
    "\n",
    "### Dummy-Encoding für den Wochentag\n",
    "\n",
    "Die `Wochentag`-Spalte wird mithilfe von `pd.get_dummies()` in separate Dummy-Variablen umgewandelt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Dummy-Encoding für Jahreszeit und Wetterkategorie**\n",
    "wochentag_dummy = pd.get_dummies(clean_data['Wochentag'], prefix=\"Wochentag\")\n",
    "warrengruppe_dummy = pd.get_dummies(clean_data['Warengruppe'], prefix=\"Warengruppe\")\n",
    "\n",
    "\n",
    "# Dummy-Variablen anhängen und Originalspalten löschen\n",
    "clean_data = pd.concat([clean_data, wochentag_dummy,warrengruppe_dummy], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umbenennen der Spalten für Wochentag und Warengruppe\n",
    "\n",
    "Nachdem wir die Dummy-Variablen für die `Wochentag`- und `Warengruppe`-Spalten erstellt haben, benennen wir die Spalten um, um eine benutzerfreundlichere Bezeichnung zu erhalten. Dies erleichtert die Interpretation der Daten.\n",
    "\n",
    "### Umbenennen der Wochentag-Spalten\n",
    "\n",
    "Die Dummy-Variablen für die Wochentage (von `Wochentag_0` bis `Wochentag_6`) werden entsprechend umbenannt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Spalte umbenennen\n",
    "clean_data.rename(columns={\"Wochentag_0\": \"Montag\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Wochentag_1\": \"Dienstag\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Wochentag_2\": \"Mittwoch\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Wochentag_3\": \"Donnerstag\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Wochentag_4\": \"Freitag\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Wochentag_5\": \"Samstag\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Wochentag_6\": \"Sonntag\"}, inplace=True)\n",
    "\n",
    "\n",
    "# Spalte umbenennen\n",
    "clean_data.rename(columns={\"Warengruppe_1\": \"Brot\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Warengruppe_2\": \"Brötchen\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Warengruppe_3\": \"Croissant\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Warengruppe_4\": \"Konditorei\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Warengruppe_5\": \"Kuchen\"}, inplace=True)\n",
    "clean_data.rename(columns={\"Warengruppe_6\": \"Saisonbrot\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umwandlung von booleschen Spalten in numerische Werte und Speichern der bereinigten Daten\n",
    "\n",
    "Im letzten Schritt des Datenvorbereitungsprozesses identifizieren wir die booleschen Spalten und konvertieren sie in numerische Werte (0 für `False` und 1 für `True`), um sie für die Analyse und Modellierung besser nutzbar zu machen. Anschließend speichern wir die bereinigten Daten in einer CSV-Datei.\n",
    "\n",
    "### Umwandlung der booleschen Spalten\n",
    "\n",
    "Zunächst identifizieren wir alle Spalten mit dem Datentyp `bool` und wandeln sie dann in den Datentyp `int` um:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify boolean columns\n",
    "boolean_columns = clean_data.select_dtypes(include=['bool']).columns\n",
    "\n",
    "# Convert boolean columns to numeric\n",
    "clean_data[boolean_columns] = clean_data[boolean_columns].astype(int)\n",
    "\n",
    "\n",
    "clean_data.to_csv(model_data_pfad, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen der CSV-Datei und Berechnung der NaN-Werte\n",
    "\n",
    "In diesem Schritt lesen wir eine CSV-Datei ein und berechnen die Gesamtanzahl der `NaN`-Werte im Datensatz. Dies ist wichtig, um sicherzustellen, dass alle fehlenden Werte erkannt und entsprechend behandelt werden können.\n",
    "\n",
    "### Einlesen der Datei\n",
    "\n",
    "Zunächst geben wir den Pfad zur Datei an und laden den Inhalt in ein pandas DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Datei enthält 0 NaN-Werte.\n"
     ]
    }
   ],
   "source": [
    "# Datei einlesen (z. B. eine CSV-Datei)\n",
    "file_path = 'final_data_test.csv'  # Pfad zur Datei\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Anzahl der NaN-Werte berechnen\n",
    "anzahl_nan = df.isna().sum().sum()\n",
    "\n",
    "print(f\"Die Datei enthält {anzahl_nan} NaN-Werte.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
